{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a90bf54-b98d-478d-a2ca-c292767ec53b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Widget Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e4665d2-7c03-45f6-a260-600429fc5918",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### parameters\n",
    "\n",
    "No. of Hyperparameter Settings (_max_evals_) : Number of hyperparameter settings to try (the number of models to fit). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b5a330-67c3-4806-9c9a-7dae8e4d5a77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_evals_widget = int(dbutils.widgets.get(\"max_evals\"))\n",
    "\n",
    "censor_training_widget = int(dbutils.widgets.get(\"censor_training\"))\n",
    "censor_validation_widget = int(dbutils.widgets.get(\"censor_validation\"))\n",
    "\n",
    "hyp_search_algo_widget = dbutils.widgets.get(\"algo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "668c7f3b-0feb-415d-a058-49e62ee3da86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark + Hyeropt\n",
    "\n",
    "Using spark enhanced version of xgboost and hyperparameter tuning with Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec87ac66-72b2-4d20-9d48-0d7e6d0e52af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c4d67a-ac34-486d-8267-c44ac6fbcee0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method DataFrame.cache of DataFrame[Pregnancies: bigint, Glucose: bigint, BloodPressure: bigint, SkinThickness: bigint, Insulin: bigint, BMI: double, DiabetesPedigreeFunction: double, Age: bigint, Outcome: bigint, abc: string]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_hive_table(schema, table_name, version):\n",
    "    \"\"\"\n",
    "    Reads Hive table into a PySpark DataFrame. \n",
    "    (lazy eval)\n",
    "\n",
    "    Args:\n",
    "        schema (str): database name\n",
    "        table_name (str): name of the Hive table.\n",
    "        version (int): table version\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: PySpark DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"versionAsOf\", version) \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"mode\", \"FAILFAST\") \\\n",
    "            .table(schema + \".\" + table_name)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading table '{table_name}': {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "df_train = read_hive_table(\"sandbox\",\"diabetestrain\",2)    \n",
    "df_test = read_hive_table(\"sandbox\",\"diabetestest\",1)    \n",
    "df_val = read_hive_table(\"sandbox\",\"diabetesvalidation\",1)   \n",
    "\n",
    "df_train.cache\n",
    "df_test.cache\n",
    "df_val.cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4789a1fd-67af-4139-a06c-cddd3dbc7a66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## data prep\n",
    "\n",
    "general data prep such as log transform our response and include a censor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a7de615-96b2-44e2-9883-2e74727e770c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_transform_response(spark_df, response_var):\n",
    "    \"\"\"\n",
    "    converts response variables using log + 1, and returns new col\n",
    "\n",
    "    Args:\n",
    "        spark_df : spark dataframe\n",
    "        response_var (str): numeric response/label/outcome variable\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Pyspark DataFrame containing a log transformed response\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import col, log1p\n",
    "\n",
    "    logTrainDF = spark_df.withColumn(f\"log_{response_var}\", log1p(col(response_var)))\n",
    "\n",
    "    return logTrainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77849270-98b1-4e16-9cff-d20c6a4bbc3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def censor(spark_df, response_var,  censor_amt=None):\n",
    "    \"\"\"\n",
    "    Replaces values over censor_amt with censor_amt, if no censor amt return dataframe without changes\n",
    "\n",
    "    Args:\n",
    "        spark_df : spark dataframe\n",
    "        response_var (str): numeric response/label/outcome variable\n",
    "        censor_amt (float/int): numeric value to set censor_amt\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: PySpark DataFrame containing the data from the specified table.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import when, col\n",
    "\n",
    "    if censor_amt is None or censor_amt == 0:\n",
    "        return spark_df\n",
    "    \n",
    "    return spark_df.withColumn(f'{response_var}', when(col(response_var) > censor_amt, censor_amt).otherwise(col(response_var)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1967ecb5-8fce-43fd-9c1b-fe630d52d179",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## spark prep\n",
    "\n",
    "\n",
    "Spark requires all data to be in numeric format and all features fit under a single column. To achieve this we use transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f19a80-f754-4c15-ac98-57839c0b6071",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def column_types(spark_df, response_var):\n",
    "    \"\"\"\n",
    "    Identifies categorical and numeric columns. Numeric columns is set to exclude response variable.\n",
    "\n",
    "    Args:\n",
    "        spark_df : spark dataframe\n",
    "        response_var (str): numeric response/label/outcome variable\n",
    "\n",
    "    Returns:\n",
    "        2 vectors holding numeric and categorical column names\n",
    "    \"\"\"    \n",
    "    categoricalCols = [field for (field, dataType) in spark_df.dtypes \n",
    "                    if dataType == \"string\"]\n",
    "\n",
    "    numericCols = [field for (field, dataType) in spark_df.dtypes \n",
    "                if ((dataType in [\"double\",\"bigint\"]) & (field != response_var or field !=\"log_\" + response_var))]\n",
    "\n",
    "    return categoricalCols, numericCols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36edf45a-5df5-418f-8d60-389ef5e823b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Training with distributed processing requires additional steps, we address these steps with components below. Requirements include only numeric type data, and all features must be saved in an array in their own individual column.\n",
    "\n",
    "Function below creates component templates (transformers) for pipeline. No actual transformations occur in this function, but rather identifies appropriate columns to be fed into transformers. Transformers include both reference to input and output columns.\n",
    "\n",
    "E.g. categorical/string col -> StringIndexer -> OheEncoder\n",
    "\n",
    "Fitting data to these requires the transform method. This will be done in the objective function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0203647f-bb31-463b-9dd9-04bfb19dfd28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def spark_prep(spark_df, response_var):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates component templates (transformers) for pipeline. Fitting data to these requires the transform method.\n",
    "\n",
    "    Components: \n",
    "    - StringIndexer: replaces categorical values with numeric (non-ordinal)\n",
    "    - OneHotEncoder: one hot encode / dummy vars\n",
    "    - VectorAssembler: creates column holding all features\n",
    "\n",
    "    Args:\n",
    "        spark_df : spark dataframe\n",
    "        response_var (str): numeric response/label/outcome variable\n",
    "\n",
    "    Returns:\n",
    "        3 transformers to be applied to our dataset\n",
    "    \"\"\"       \n",
    "\n",
    "    from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "    cat_cols, num_cols = column_types(spark_df=spark_df, response_var=response_var)\n",
    "\n",
    "    indexOutputCols = [x + \"Index\" for x in cat_cols]\n",
    "\n",
    "    oheOutputCols = [x + \"OHE\" for x in cat_cols]\n",
    "\n",
    "    stringIndexer = StringIndexer(inputCols=cat_cols, \n",
    "                                outputCols=indexOutputCols, \n",
    "                                handleInvalid=\"skip\")\n",
    "    oheEncoder = OneHotEncoder(inputCols=indexOutputCols, \n",
    "                            outputCols=oheOutputCols)\n",
    "\n",
    "    # combine untransformed numeric columns with our ohe encoded columns\n",
    "    assemblerInputs = oheOutputCols + num_cols\n",
    "\n",
    "    vecAssembler = VectorAssembler(inputCols=assemblerInputs, \n",
    "                                outputCol=\"features\")\n",
    "\n",
    "    return stringIndexer, oheEncoder, vecAssembler         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6ef24d-fa1c-4120-a79a-d8b83963c6fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# define hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121a62c3-886d-449a-a21b-5a51809c9c76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "search_space = {\n",
    "  'learning_rate': hp.uniform('learning_rate', .01, 1),\n",
    "  'max_depth': hp.quniform('max_depth', 2, 10, 2),\n",
    "  'min_child_weight': hp.quniform('min_child_weight', .2, 10, 2),\n",
    "  'subsample': hp.uniform('subsample', .01, 1),\n",
    "  'colsample_bytree': hp.uniform('colsample_bytree', .01, 1),\n",
    "  'gamma': hp.choice('gamma', [0,1]),\n",
    "  'scale_pos_weight': hp.choice('scale_pos_weight', [1]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21cb163-2896-4c14-a98e-1f00416cbd61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## define objective function\n",
    "\n",
    "This will be fed into hyperopts _fmin()_ function. This step is what ties all our preprocessing (censoring, log transform), transformer components (stringIndexer, oheEncoder, vecAssembler), and model object (xgb_regressor) given different combinations of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a64fa76-d252-4bf0-a696-d6a7f5fa830c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from hyperopt import STATUS_OK\n",
    "from pyspark.sql.functions import col, exp\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    response_var = \"BloodPressure\"\n",
    "    # Initialize the XGBoost regressor with current parameters\n",
    "    xgb_regressor = SparkXGBRegressor(\n",
    "        features_col=\"features\",\n",
    "        label_col=f\"log_{response_var}\",\n",
    "        prediction_col=\"log_prediction\",\n",
    "        objective=\"reg:squarederror\", # adjust according to your needs\n",
    "        numWorkers=4,  # Adjust according to your cluster\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=int(params['max_depth']),\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        gamma=int(params['gamma']),\n",
    "        scale_pos_weight=int(params['scale_pos_weight'])        \n",
    "    )\n",
    "\n",
    "    # censor - training data\n",
    "    censored_df_train = censor(spark_df = df_train, response_var = response_var,  censor_amt=censor_training_widget)\n",
    "\n",
    "    # censor - validation data\n",
    "    censored_df_val = censor(spark_df = df_val, response_var = response_var,  censor_amt=censor_validation_widget)\n",
    "\n",
    "    # log transform response - training data\n",
    "    log_df_train =log_transform_response(spark_df = censored_df_train, response_var = response_var)\n",
    "\n",
    "    # log transform response - validation data\n",
    "    log_df_val = log_transform_response(spark_df = censored_df_val, response_var = response_var)   \n",
    "\n",
    "    # define transformers\n",
    "    stringIndexer, oheEncoder, vecAssembler = spark_prep(spark_df = log_df_train , response_var = response_var)\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline(stages=[stringIndexer, oheEncoder, vecAssembler, xgb_regressor])\n",
    "    \n",
    "    # Train the model\n",
    "    model = pipeline.fit(log_df_train)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.transform(log_df_val)\n",
    "    \n",
    "    # Exponentiate\n",
    "    exp_predictions = predictions.withColumn(\"prediction\", exp(col(\"log_prediction\"))-1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=response_var, \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"\n",
    "    )\n",
    "    \n",
    "    mae = evaluator.evaluate(exp_predictions)\n",
    "    \n",
    "    # Hyperopt minimizes the objective, so return RMSE as a loss to minimize\n",
    "    return {'loss': mae, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53f499e-1f03-4c33-ad5f-3c8f7c18e743",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## select a search algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8503dd69-c32b-4a71-804b-0291874eb724",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import tpe, rand\n",
    "\n",
    "def hyper_search_algorithm(widget):\n",
    "    if hyp_search_algo_widget == \"bayesian\":\n",
    "        return tpe.suggest\n",
    "    else:\n",
    "        return rand.suggest\n",
    "    \n",
    "algo = hyper_search_algorithm(widget=hyp_search_algo_widget)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867c4b95-29be-4dcb-82d2-ec1a983fc49c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## run hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4674b1ad-1b2d-4307-a8a2-5dfbc20ee2e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r 20%|██        | 1/5 [00:07<00:31,  7.83s/trial, best loss: 1.410969015237947]\r 40%|████      | 2/5 [00:14<00:21,  7.32s/trial, best loss: 0.5332417943669792]\r 60%|██████    | 3/5 [00:21<00:14,  7.26s/trial, best loss: 0.5332417943669792]\r 80%|████████  | 4/5 [00:28<00:06,  6.98s/trial, best loss: 0.5332417943669792]\r100%|██████████| 5/5 [00:34<00:00,  6.75s/trial, best loss: 0.21200174272823794]\r100%|██████████| 5/5 [00:34<00:00,  6.97s/trial, best loss: 0.21200174272823794]\nBest hyperparameters:{'colsample_bytree': 0.2651387398101446, 'gamma': 1, 'learning_rate': 0.8293317629024581, 'max_depth': 10.0, 'min_child_weight': 0.0, 'scale_pos_weight': 0, 'subsample': 0.20325259608635998}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, Trials, SparkTrials\n",
    "\n",
    "# Run the hyperparameter search using the Tree of Parzen Estimators (TPE) algorithm\n",
    "#trials = SparkTrials(parallelism=1)\n",
    "trials = Trials() # using Trials because we are using Spark Xgboost\n",
    "\n",
    "best_params = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=algo,\n",
    "    max_evals=max_evals_widget,  # Adjust based on how long you're willing to wait\n",
    "    trials=trials,\n",
    "    #loss_threshold=0.9999999,\n",
    "    #timeout=60*100\n",
    ")\n",
    "\n",
    "print(f\"Best hyperparameters:{best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6032725f-865b-4db1-8d33-b6c4b79ecc63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'state': 2,\n",
       " 'tid': 4,\n",
       " 'spec': None,\n",
       " 'result': {'loss': 0.21200174272823794, 'status': 'ok'},\n",
       " 'misc': {'tid': 4,\n",
       "  'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'workdir': None,\n",
       "  'idxs': {'colsample_bytree': [4],\n",
       "   'gamma': [4],\n",
       "   'learning_rate': [4],\n",
       "   'max_depth': [4],\n",
       "   'min_child_weight': [4],\n",
       "   'scale_pos_weight': [4],\n",
       "   'subsample': [4]},\n",
       "  'vals': {'colsample_bytree': [0.2651387398101446],\n",
       "   'gamma': [1],\n",
       "   'learning_rate': [0.8293317629024581],\n",
       "   'max_depth': [10.0],\n",
       "   'min_child_weight': [0.0],\n",
       "   'scale_pos_weight': [0],\n",
       "   'subsample': [0.20325259608635998]}},\n",
       " 'exp_key': None,\n",
       " 'owner': None,\n",
       " 'version': 0,\n",
       " 'book_time': datetime.datetime(2024, 4, 19, 22, 58, 27, 163000),\n",
       " 'refresh_time': datetime.datetime(2024, 4, 19, 22, 58, 33, 493000)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.best_trial"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "algo",
      "width": 272
     },
     {
      "breakBefore": false,
      "name": "max_evals",
      "width": 272
     },
     {
      "breakBefore": false,
      "name": "censor_amt",
      "width": 200
     }
    ]
   },
   "notebookName": "Spark_Hyeropt_dev_v1",
   "widgets": {
    "algo": {
     "currentValue": "bayesian",
     "nuid": "9d06a271-9086-4212-b228-0588fddb4895",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "bayesian",
      "label": "Hyperparameter Search Algorithm",
      "name": "algo",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "bayesian",
        "random"
       ]
      }
     }
    },
    "censor_training": {
     "currentValue": "50",
     "nuid": "0dcf90c9-cd2c-4197-9c03-94a7bfff55bc",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0",
      "label": "censor training",
      "name": "censor_training",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "censor_validation": {
     "currentValue": "50",
     "nuid": "b1c05015-aa75-4473-b359-511a47c49244",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0",
      "label": "censor validation",
      "name": "censor_validation",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "max_evals": {
     "currentValue": "5",
     "nuid": "8eeefc8e-b510-402f-875d-9f92cb7aeafa",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5",
      "label": "No. of Hyperparameter Settings",
      "name": "max_evals",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
